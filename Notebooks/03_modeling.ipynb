{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7652c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SaaS PRICING MODEL SELECTION - MACHINE LEARNING MODELS\n",
      "================================================================================\n",
      "\n",
      "1. LOADING CLEANED DATASETS...\n",
      "✓ Ravenstack: (5000, 26)\n",
      "✓ CAC-LTV: (7057, 15)\n",
      "✓ SaaS Businesses: (126, 21)\n",
      "\n",
      "================================================================================\n",
      "2. FEATURE ENGINEERING\n",
      "================================================================================\n",
      "\n",
      "2.1 Preparing Ravenstack features...\n",
      "  - Numerical features: 7\n",
      "  - Categorical features: 12\n",
      "\n",
      "2.2 Engineering CAC-LTV features...\n",
      "\n",
      "================================================================================\n",
      "3. MODEL 1: LINEAR REGRESSION - PRICE OPTIMIZATION\n",
      "================================================================================\n",
      "\n",
      "3.1 Preparing data for Linear Regression...\n",
      "  ⚠ No price column found in Ravenstack dataset\n",
      "\n",
      "================================================================================\n",
      "4. MODEL 2: POLYNOMIAL REGRESSION\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "5. MODEL 3: DECISION TREE - CHURN PREDICTION\n",
      "================================================================================\n",
      "\n",
      "5.1 Preparing data for Decision Tree (target: churn_flag_x)...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'scaler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 270\u001b[39m\n\u001b[32m    265\u001b[39m X_train_dt, X_test_dt, y_train_dt, y_test_dt = train_test_split(\n\u001b[32m    266\u001b[39m     X_dt, y_dt, test_size=\u001b[32m0.2\u001b[39m, random_state=\u001b[32m42\u001b[39m, stratify=y_dt\n\u001b[32m    267\u001b[39m )\n\u001b[32m    269\u001b[39m \u001b[38;5;66;03m# Scale features\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m270\u001b[39m X_train_dt_scaled = \u001b[43mscaler\u001b[49m.fit_transform(X_train_dt)\n\u001b[32m    271\u001b[39m X_test_dt_scaled = scaler.transform(X_test_dt)\n\u001b[32m    273\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Features: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(dt_feature_cols)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'scaler' is not defined"
     ]
    }
   ],
   "source": [
    "# SaaS Pricing Model - Machine Learning Models\n",
    "# Student: Sanskriti Avinash Dabhade (1225131)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning imports\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, plot_tree\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import (mean_squared_error, r2_score, mean_absolute_error,\n",
    "                            accuracy_score, precision_score, recall_score, f1_score,\n",
    "                            confusion_matrix, classification_report, silhouette_score)\n",
    "\n",
    "# Set styling\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"SaaS PRICING MODEL SELECTION - MACHINE LEARNING MODELS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 1: LOAD AND PREPARE DATA\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n1. LOADING CLEANED DATASETS...\")\n",
    "\n",
    "ravenstack = pd.read_csv('Cleaned Data/ravenstack_cleaned.csv')\n",
    "cac_ltv = pd.read_csv('Cleaned Data/cac_ltv_cleaned.csv')\n",
    "saas_businesses = pd.read_csv('Cleaned Data/saas_businesses_cleaned.csv')\n",
    "\n",
    "print(f\"✓ Ravenstack: {ravenstack.shape}\")\n",
    "print(f\"✓ CAC-LTV: {cac_ltv.shape}\")\n",
    "print(f\"✓ SaaS Businesses: {saas_businesses.shape}\")\n",
    "\n",
    "# Create results directory\n",
    "results_dir = Path('Model Results')\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 2: FEATURE ENGINEERING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"2. FEATURE ENGINEERING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 2.1 Ravenstack - Prepare features for modeling\n",
    "print(\"\\n2.1 Preparing Ravenstack features...\")\n",
    "\n",
    "# Select numerical features\n",
    "numerical_features = ravenstack.select_dtypes(include=[np.number]).columns.tolist()\n",
    "print(f\"  - Numerical features: {len(numerical_features)}\")\n",
    "\n",
    "# Encode categorical variables\n",
    "categorical_features = ravenstack.select_dtypes(include=['object']).columns.tolist()\n",
    "print(f\"  - Categorical features: {len(categorical_features)}\")\n",
    "\n",
    "# Create label encoders for categorical variables\n",
    "label_encoders = {}\n",
    "for col in categorical_features:\n",
    "    if ravenstack[col].nunique() < 50:  # Only encode if not too many unique values\n",
    "        le = LabelEncoder()\n",
    "        ravenstack[f'{col}_encoded'] = le.fit_transform(ravenstack[col].astype(str))\n",
    "        label_encoders[col] = le\n",
    "\n",
    "# 2.2 CAC-LTV - Feature engineering\n",
    "print(\"\\n2.2 Engineering CAC-LTV features...\")\n",
    "\n",
    "# Calculate LTV/CAC ratio if columns exist\n",
    "cac_cols = [col for col in cac_ltv.columns if 'cac' in col.lower()]\n",
    "ltv_cols = [col for col in cac_ltv.columns if 'ltv' in col.lower()]\n",
    "\n",
    "if len(cac_cols) > 0 and len(ltv_cols) > 0:\n",
    "    cac_ltv['ltv_cac_ratio'] = cac_ltv[ltv_cols[0]] / (cac_ltv[cac_cols[0]] + 1)  # Add 1 to avoid division by zero\n",
    "    cac_ltv['is_healthy'] = (cac_ltv['ltv_cac_ratio'] >= 3).astype(int)\n",
    "    print(f\"  ✓ Created LTV/CAC ratio and health indicator\")\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 3: MODEL 1 - LINEAR REGRESSION (Price Optimization)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"3. MODEL 1: LINEAR REGRESSION - PRICE OPTIMIZATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Identify target and features for pricing model\n",
    "print(\"\\n3.1 Preparing data for Linear Regression...\")\n",
    "\n",
    "# Find price-related column\n",
    "price_cols = [col for col in ravenstack.columns if 'price' in col.lower() or 'revenue' in col.lower()]\n",
    "if len(price_cols) > 0:\n",
    "    target_col = price_cols[0]\n",
    "    print(f\"  Target variable: {target_col}\")\n",
    "    \n",
    "    # Select features (numerical only for simplicity)\n",
    "    feature_cols = [col for col in numerical_features if col != target_col and col in ravenstack.columns][:10]\n",
    "    \n",
    "    # Remove rows with missing values in target or features\n",
    "    lr_data = ravenstack[[target_col] + feature_cols].dropna()\n",
    "    \n",
    "    print(f\"  Features: {len(feature_cols)}\")\n",
    "    print(f\"  Samples: {len(lr_data)}\")\n",
    "    \n",
    "    # Split data\n",
    "    X = lr_data[feature_cols]\n",
    "    y = lr_data[target_col]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    print(\"\\n3.2 Training Linear Regression model...\")\n",
    "    lr_model = LinearRegression()\n",
    "    lr_model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred_train = lr_model.predict(X_train_scaled)\n",
    "    y_pred_test = lr_model.predict(X_test_scaled)\n",
    "    \n",
    "    # Evaluation\n",
    "    print(\"\\n3.3 Linear Regression Results:\")\n",
    "    print(f\"  Train R²: {r2_score(y_train, y_pred_train):.4f}\")\n",
    "    print(f\"  Test R²: {r2_score(y_test, y_pred_test):.4f}\")\n",
    "    print(f\"  Train RMSE: {np.sqrt(mean_squared_error(y_train, y_pred_train)):.4f}\")\n",
    "    print(f\"  Test RMSE: {np.sqrt(mean_squared_error(y_test, y_pred_test)):.4f}\")\n",
    "    print(f\"  Test MAE: {mean_absolute_error(y_test, y_pred_test):.4f}\")\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(lr_model, X_train_scaled, y_train, cv=5, scoring='r2')\n",
    "    print(f\"  Cross-validation R² (mean ± std): {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
    "    \n",
    "    # Feature importance\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': feature_cols,\n",
    "        'Coefficient': lr_model.coef_\n",
    "    }).sort_values('Coefficient', key=abs, ascending=False)\n",
    "    \n",
    "    print(\"\\n3.4 Top 5 Most Important Features:\")\n",
    "    print(feature_importance.head())\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Plot 1: Actual vs Predicted\n",
    "    ax1 = axes[0, 0]\n",
    "    ax1.scatter(y_test, y_pred_test, alpha=0.5, color='blue')\n",
    "    ax1.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "    ax1.set_xlabel('Actual Price')\n",
    "    ax1.set_ylabel('Predicted Price')\n",
    "    ax1.set_title('Linear Regression: Actual vs Predicted', fontweight='bold')\n",
    "    ax1.grid(alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Residuals\n",
    "    ax2 = axes[0, 1]\n",
    "    residuals = y_test - y_pred_test\n",
    "    ax2.scatter(y_pred_test, residuals, alpha=0.5, color='green')\n",
    "    ax2.axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "    ax2.set_xlabel('Predicted Price')\n",
    "    ax2.set_ylabel('Residuals')\n",
    "    ax2.set_title('Residual Plot', fontweight='bold')\n",
    "    ax2.grid(alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Feature importance\n",
    "    ax3 = axes[1, 0]\n",
    "    top_features = feature_importance.head(10)\n",
    "    ax3.barh(range(len(top_features)), top_features['Coefficient'].abs())\n",
    "    ax3.set_yticks(range(len(top_features)))\n",
    "    ax3.set_yticklabels(top_features['Feature'])\n",
    "    ax3.set_xlabel('Absolute Coefficient Value')\n",
    "    ax3.set_title('Feature Importance (Top 10)', fontweight='bold')\n",
    "    ax3.grid(alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Distribution of residuals\n",
    "    ax4 = axes[1, 1]\n",
    "    ax4.hist(residuals, bins=50, edgecolor='black', color='orange')\n",
    "    ax4.set_xlabel('Residual')\n",
    "    ax4.set_ylabel('Frequency')\n",
    "    ax4.set_title('Distribution of Residuals', fontweight='bold')\n",
    "    ax4.axvline(x=0, color='r', linestyle='--', lw=2)\n",
    "    ax4.grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('Model Results/linear_regression_results.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"\\n✓ Saved: Model Results/linear_regression_results.png\")\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"  ⚠ No price column found in Ravenstack dataset\")\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 4: MODEL 2 - POLYNOMIAL REGRESSION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"4. MODEL 2: POLYNOMIAL REGRESSION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if len(price_cols) > 0:\n",
    "    print(\"\\n4.1 Training Polynomial Regression (degree=2)...\")\n",
    "    \n",
    "    # Create polynomial features\n",
    "    poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "    X_train_poly = poly.fit_transform(X_train_scaled)\n",
    "    X_test_poly = poly.transform(X_test_scaled)\n",
    "    \n",
    "    # Train model\n",
    "    poly_model = LinearRegression()\n",
    "    poly_model.fit(X_train_poly, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred_train_poly = poly_model.predict(X_train_poly)\n",
    "    y_pred_test_poly = poly_model.predict(X_test_poly)\n",
    "    \n",
    "    # Evaluation\n",
    "    print(\"\\n4.2 Polynomial Regression Results:\")\n",
    "    print(f\"  Train R²: {r2_score(y_train, y_pred_train_poly):.4f}\")\n",
    "    print(f\"  Test R²: {r2_score(y_test, y_pred_test_poly):.4f}\")\n",
    "    print(f\"  Train RMSE: {np.sqrt(mean_squared_error(y_train, y_pred_train_poly)):.4f}\")\n",
    "    print(f\"  Test RMSE: {np.sqrt(mean_squared_error(y_test, y_pred_test_poly)):.4f}\")\n",
    "    print(f\"  Test MAE: {mean_absolute_error(y_test, y_pred_test_poly):.4f}\")\n",
    "    \n",
    "    print(\"\\n4.3 Comparison: Linear vs Polynomial Regression\")\n",
    "    print(f\"  Linear R²: {r2_score(y_test, y_pred_test):.4f}\")\n",
    "    print(f\"  Polynomial R²: {r2_score(y_test, y_pred_test_poly):.4f}\")\n",
    "    print(f\"  Improvement: {(r2_score(y_test, y_pred_test_poly) - r2_score(y_test, y_pred_test)):.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 5: MODEL 3 - DECISION TREE (Churn Classification)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"5. MODEL 3: DECISION TREE - CHURN PREDICTION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Find churn-related column\n",
    "churn_cols = [col for col in ravenstack.columns if 'churn' in col.lower()]\n",
    "\n",
    "if len(churn_cols) > 0:\n",
    "    churn_col = churn_cols[0]\n",
    "    print(f\"\\n5.1 Preparing data for Decision Tree (target: {churn_col})...\")\n",
    "    \n",
    "    # Prepare features\n",
    "    dt_feature_cols = [col for col in numerical_features if col != churn_col and col in ravenstack.columns][:15]\n",
    "    dt_data = ravenstack[[churn_col] + dt_feature_cols].dropna()\n",
    "    \n",
    "    # Ensure binary classification\n",
    "    if dt_data[churn_col].nunique() == 2:\n",
    "        X_dt = dt_data[dt_feature_cols]\n",
    "        y_dt = dt_data[churn_col]\n",
    "        \n",
    "        # Split data\n",
    "        X_train_dt, X_test_dt, y_train_dt, y_test_dt = train_test_split(\n",
    "            X_dt, y_dt, test_size=0.2, random_state=42, stratify=y_dt\n",
    "        )\n",
    "        \n",
    "        # Scale features\n",
    "        scaler_dt = StandardScaler()\n",
    "        X_train_dt_scaled = scaler_dt.fit_transform(X_train_dt)\n",
    "        X_test_dt_scaled = scaler_dt.transform(X_test_dt)\n",
    "        \n",
    "        print(f\"  Features: {len(dt_feature_cols)}\")\n",
    "        print(f\"  Samples: {len(dt_data)}\")\n",
    "        print(f\"  Class distribution: {y_dt.value_counts().to_dict()}\")\n",
    "        \n",
    "        print(\"\\n5.2 Training Decision Tree Classifier...\")\n",
    "        dt_model = DecisionTreeClassifier(max_depth=5, min_samples_split=20, random_state=42)\n",
    "        dt_model.fit(X_train_dt_scaled, y_train_dt)\n",
    "        \n",
    "        # Predictions\n",
    "        y_pred_train_dt = dt_model.predict(X_train_dt_scaled)\n",
    "        y_pred_test_dt = dt_model.predict(X_test_dt_scaled)\n",
    "        \n",
    "        # Evaluation\n",
    "        print(\"\\n5.3 Decision Tree Results:\")\n",
    "        print(f\"  Train Accuracy: {accuracy_score(y_train_dt, y_pred_train_dt):.4f}\")\n",
    "        print(f\"  Test Accuracy: {accuracy_score(y_test_dt, y_pred_test_dt):.4f}\")\n",
    "        print(f\"  Test Precision: {precision_score(y_test_dt, y_pred_test_dt, average='weighted', zero_division=0):.4f}\")\n",
    "        print(f\"  Test Recall: {recall_score(y_test_dt, y_pred_test_dt, average='weighted', zero_division=0):.4f}\")\n",
    "        print(f\"  Test F1-Score: {f1_score(y_test_dt, y_pred_test_dt, average='weighted', zero_division=0):.4f}\")\n",
    "        \n",
    "        # Cross-validation\n",
    "        cv_scores_dt = cross_val_score(dt_model, X_train_dt_scaled, y_train_dt, cv=5, scoring='accuracy')\n",
    "        print(f\"  Cross-validation Accuracy (mean ± std): {cv_scores_dt.mean():.4f} ± {cv_scores_dt.std():.4f}\")\n",
    "        \n",
    "        print(\"\\n5.4 Classification Report:\")\n",
    "        print(classification_report(y_test_dt, y_pred_test_dt, zero_division=0))\n",
    "        \n",
    "        # Feature importance\n",
    "        dt_feature_importance = pd.DataFrame({\n",
    "            'Feature': dt_feature_cols,\n",
    "            'Importance': dt_model.feature_importances_\n",
    "        }).sort_values('Importance', ascending=False)\n",
    "        \n",
    "        print(\"\\n5.5 Top 5 Most Important Features:\")\n",
    "        print(dt_feature_importance.head())\n",
    "        \n",
    "        # Visualizations\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # Plot 1: Confusion Matrix\n",
    "        ax1 = axes[0, 0]\n",
    "        cm = confusion_matrix(y_test_dt, y_pred_test_dt)\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax1)\n",
    "        ax1.set_xlabel('Predicted')\n",
    "        ax1.set_ylabel('Actual')\n",
    "        ax1.set_title('Confusion Matrix', fontweight='bold')\n",
    "        \n",
    "        # Plot 2: Feature Importance\n",
    "        ax2 = axes[0, 1]\n",
    "        top_dt_features = dt_feature_importance.head(10)\n",
    "        ax2.barh(range(len(top_dt_features)), top_dt_features['Importance'])\n",
    "        ax2.set_yticks(range(len(top_dt_features)))\n",
    "        ax2.set_yticklabels(top_dt_features['Feature'])\n",
    "        ax2.set_xlabel('Importance')\n",
    "        ax2.set_title('Feature Importance (Top 10)', fontweight='bold')\n",
    "        ax2.grid(alpha=0.3)\n",
    "        \n",
    "        # Plot 3: Decision Tree Visualization (simplified)\n",
    "        ax3 = axes[1, 0]\n",
    "        plot_tree(dt_model, max_depth=2, feature_names=dt_feature_cols[:10], \n",
    "                 filled=True, ax=ax3, fontsize=8)\n",
    "        ax3.set_title('Decision Tree Structure (depth=2)', fontweight='bold')\n",
    "        \n",
    "        # Plot 4: Class distribution\n",
    "        ax4 = axes[1, 1]\n",
    "        class_counts = pd.DataFrame({\n",
    "            'Actual': y_test_dt.value_counts(),\n",
    "            'Predicted': pd.Series(y_pred_test_dt).value_counts()\n",
    "        })\n",
    "        class_counts.plot(kind='bar', ax=ax4, color=['steelblue', 'coral'])\n",
    "        ax4.set_xlabel('Class')\n",
    "        ax4.set_ylabel('Count')\n",
    "        ax4.set_title('Actual vs Predicted Class Distribution', fontweight='bold')\n",
    "        ax4.tick_params(axis='x', rotation=0)\n",
    "        ax4.legend()\n",
    "        ax4.grid(alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('Model Results/decision_tree_results.png', dpi=300, bbox_inches='tight')\n",
    "        print(\"\\n✓ Saved: Model Results/decision_tree_results.png\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"  ⚠ Churn column has {dt_data[churn_col].nunique()} unique values (expected 2 for binary classification)\")\n",
    "else:\n",
    "    print(\"  ⚠ No churn column found in Ravenstack dataset\")\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 6: MODEL 4 - K-MEANS CLUSTERING (Customer Segmentation)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"6. MODEL 4: K-MEANS CLUSTERING - CUSTOMER SEGMENTATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n6.1 Preparing data for K-Means clustering...\")\n",
    "\n",
    "# Select numerical features for clustering\n",
    "cluster_features = ravenstack.select_dtypes(include=[np.number]).columns.tolist()[:10]\n",
    "cluster_data = ravenstack[cluster_features].dropna()\n",
    "\n",
    "print(f\"  Features: {len(cluster_features)}\")\n",
    "print(f\"  Samples: {len(cluster_data)}\")\n",
    "\n",
    "# Scale features\n",
    "scaler_kmeans = StandardScaler()\n",
    "X_cluster = scaler_kmeans.fit_transform(cluster_data)\n",
    "\n",
    "# Determine optimal number of clusters using elbow method\n",
    "print(\"\\n6.2 Finding optimal number of clusters...\")\n",
    "inertias = []\n",
    "silhouette_scores = []\n",
    "K_range = range(2, 11)\n",
    "\n",
    "for k in K_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(X_cluster)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "    silhouette_scores.append(silhouette_score(X_cluster, kmeans.labels_))\n",
    "\n",
    "# Train final model with optimal k\n",
    "optimal_k = 4  # Can be adjusted based on elbow plot\n",
    "print(f\"\\n6.3 Training K-Means with k={optimal_k}...\")\n",
    "\n",
    "kmeans_model = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "clusters = kmeans_model.fit_predict(X_cluster)\n",
    "\n",
    "# Add cluster labels to data\n",
    "cluster_data['Cluster'] = clusters\n",
    "\n",
    "# Evaluation\n",
    "print(f\"\\n6.4 K-Means Clustering Results:\")\n",
    "print(f\"  Optimal K: {optimal_k}\")\n",
    "print(f\"  Inertia: {kmeans_model.inertia_:.2f}\")\n",
    "print(f\"  Silhouette Score: {silhouette_score(X_cluster, clusters):.4f}\")\n",
    "\n",
    "print(f\"\\n6.5 Cluster Distribution:\")\n",
    "print(pd.Series(clusters).value_counts().sort_index())\n",
    "\n",
    "# Cluster statistics\n",
    "print(f\"\\n6.6 Cluster Characteristics:\")\n",
    "cluster_summary = cluster_data.groupby('Cluster')[cluster_features].mean()\n",
    "print(cluster_summary)\n",
    "\n",
    "# Visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Plot 1: Elbow plot\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(K_range, inertias, 'bo-')\n",
    "ax1.set_xlabel('Number of Clusters (K)')\n",
    "ax1.set_ylabel('Inertia')\n",
    "ax1.set_title('Elbow Method', fontweight='bold')\n",
    "ax1.grid(alpha=0.3)\n",
    "ax1.axvline(x=optimal_k, color='r', linestyle='--', label=f'Optimal K={optimal_k}')\n",
    "ax1.legend()\n",
    "\n",
    "# Plot 2: Silhouette scores\n",
    "ax2 = axes[0, 1]\n",
    "ax2.plot(K_range, silhouette_scores, 'go-')\n",
    "ax2.set_xlabel('Number of Clusters (K)')\n",
    "ax2.set_ylabel('Silhouette Score')\n",
    "ax2.set_title('Silhouette Score Analysis', fontweight='bold')\n",
    "ax2.grid(alpha=0.3)\n",
    "ax2.axvline(x=optimal_k, color='r', linestyle='--', label=f'Optimal K={optimal_k}')\n",
    "ax2.legend()\n",
    "\n",
    "# Plot 3: Cluster visualization (first 2 PCA components)\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_cluster)\n",
    "\n",
    "ax3 = axes[1, 0]\n",
    "scatter = ax3.scatter(X_pca[:, 0], X_pca[:, 1], c=clusters, cmap='viridis', alpha=0.6)\n",
    "ax3.scatter(pca.transform(kmeans_model.cluster_centers_)[:, 0],\n",
    "           pca.transform(kmeans_model.cluster_centers_)[:, 1],\n",
    "           c='red', marker='X', s=200, edgecolors='black', label='Centroids')\n",
    "ax3.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\n",
    "ax3.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\n",
    "ax3.set_title('Customer Segments (PCA Visualization)', fontweight='bold')\n",
    "ax3.legend()\n",
    "plt.colorbar(scatter, ax=ax3, label='Cluster')\n",
    "\n",
    "# Plot 4: Cluster sizes\n",
    "ax4 = axes[1, 1]\n",
    "cluster_counts = pd.Series(clusters).value_counts().sort_index()\n",
    "ax4.bar(cluster_counts.index, cluster_counts.values, color='teal')\n",
    "ax4.set_xlabel('Cluster')\n",
    "ax4.set_ylabel('Number of Customers')\n",
    "ax4.set_title('Cluster Size Distribution', fontweight='bold')\n",
    "ax4.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('Model Results/kmeans_clustering_results.png', dpi=300, bbox_inches='tight')\n",
    "print(\"\\n✓ Saved: Model Results/kmeans_clustering_results.png\")\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 7: INDUSTRY SEGMENTATION (SaaS Businesses Dataset)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"7. INDUSTRY SEGMENTATION - SAAS BUSINESSES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n7.1 Clustering SaaS companies by characteristics...\")\n",
    "\n",
    "# Prepare numerical features from SaaS businesses\n",
    "business_numerical = saas_businesses.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "if len(business_numerical) > 0:\n",
    "    business_cluster_data = saas_businesses[business_numerical].dropna()\n",
    "    \n",
    "    if len(business_cluster_data) > optimal_k:\n",
    "        # Scale features\n",
    "        X_business = scaler_kmeans.fit_transform(business_cluster_data)\n",
    "        \n",
    "        # Cluster\n",
    "        business_kmeans = KMeans(n_clusters=min(optimal_k, len(business_cluster_data)//10), \n",
    "                                random_state=42, n_init=10)\n",
    "        business_clusters = business_kmeans.fit_predict(X_business)\n",
    "        \n",
    "        print(f\"  Features: {len(business_numerical)}\")\n",
    "        print(f\"  Samples: {len(business_cluster_data)}\")\n",
    "        print(f\"  Silhouette Score: {silhouette_score(X_business, business_clusters):.4f}\")\n",
    "        \n",
    "        print(f\"\\n7.2 Business Cluster Distribution:\")\n",
    "        print(pd.Series(business_clusters).value_counts().sort_index())\n",
    "    else:\n",
    "        print(\"  ⚠ Insufficient samples for clustering\")\n",
    "else:\n",
    "    print(\"  ⚠ No numerical features found in SaaS businesses dataset\")\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 8: MODEL COMPARISON SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"8. MODEL PERFORMANCE SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create summary table\n",
    "summary_data = {\n",
    "    'Model': [],\n",
    "    'Task': [],\n",
    "    'Primary Metric': [],\n",
    "    'Score': [],\n",
    "    'Status': []\n",
    "}\n",
    "\n",
    "if len(price_cols) > 0:\n",
    "    summary_data['Model'].append('Linear Regression')\n",
    "    summary_data['Task'].append('Price Prediction')\n",
    "    summary_data['Primary Metric'].append('Test R²')\n",
    "    summary_data['Score'].append(f\"{r2_score(y_test, y_pred_test):.4f}\")\n",
    "    summary_data['Status'].append('✓ Complete')\n",
    "    \n",
    "    summary_data['Model'].append('Polynomial Regression')\n",
    "    summary_data['Task'].append('Price Prediction')\n",
    "    summary_data['Primary Metric'].append('Test R²')\n",
    "    summary_data['Score'].append(f\"{r2_score(y_test, y_pred_test_poly):.4f}\")\n",
    "    summary_data['Status'].append('✓ Complete')\n",
    "\n",
    "if len(churn_cols) > 0 and 'y_test_dt' in locals():\n",
    "    summary_data['Model'].append('Decision Tree')\n",
    "    summary_data['Task'].append('Churn Classification')\n",
    "    summary_data['Primary Metric'].append('Test Accuracy')\n",
    "    summary_data['Score'].append(f\"{accuracy_score(y_test_dt, y_pred_test_dt):.4f}\")\n",
    "    summary_data['Status'].append('✓ Complete')\n",
    "\n",
    "summary_data['Model'].append('K-Means Clustering')\n",
    "summary_data['Task'].append('Customer Segmentation')\n",
    "summary_data['Primary Metric'].append('Silhouette Score')\n",
    "summary_data['Score'].append(f\"{silhouette_score(X_cluster, clusters):.4f}\")\n",
    "summary_data['Status'].append('✓ Complete')\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"\\n\" + summary_df.to_string(index=False))\n",
    "\n",
    "# Save summary\n",
    "summary_df.to_csv('Model Results/model_summary.csv', index=False)\n",
    "print(\"\\n✓ Saved: Model Results/model_summary.csv\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"✓ ALL MODELS TRAINED AND EVALUATED SUCCESSFULLY!\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nKey Findings:\")\n",
    "print(\"  1. Pricing models help identify optimal price points\")\n",
    "print(\"  2. Churn prediction identifies at-risk customers\")\n",
    "print(\"  3. Customer segmentation reveals distinct user groups\")\n",
    "print(\"  4. Industry clustering shows pricing strategy variations\")\n",
    "\n",
    "print(\"\\nNext Steps:\")\n",
    "print(\"  • Validate findings with CAC-LTV financial metrics\")\n",
    "print(\"  • Deploy best-performing models\")\n",
    "print(\"  • Create pricing recommendations framework\")\n",
    "print(\"  • Monitor model performance over time\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
